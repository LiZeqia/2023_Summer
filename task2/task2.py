# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11-X2gg8pWOsi0Q4gKLZhGu6Opug4kqhg
"""


import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
import gradio as gr
import torchvision
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
import numpy as np
import plotly.graph_objs as go
import logging


#resnet18网络
class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += self.shortcut(x)
        out = self.relu(out)
        return out

class ResNet18(nn.Module):
    def __init__(self, num_classes=1000):
        super(ResNet18, self).__init__()
        self.in_channels = 64
        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self.make_layer(64, 2, stride=1)
        self.layer2 = self.make_layer(128, 2, stride=2)
        self.layer3 = self.make_layer(256, 2, stride=2)
        self.layer4 = self.make_layer(512, 2, stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)

    def make_layer(self, out_channels, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(ResidualBlock(self.in_channels, out_channels, stride))
            self.in_channels = out_channels
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.maxpool(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

#移除残差连接
class ResidualBlockNoRes(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1):
        super(ResidualBlockNoRes, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.shortcut = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.shortcut = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )

    def forward(self, x):
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        # 删除以下这一行以移除残差连接
        # out += self.shortcut(x)
        out = self.relu(out)
        return out

class ResNet18NoRes(ResNet18):
    def make_layer(self, out_channels, num_blocks, stride):
        strides = [stride] + [1] * (num_blocks - 1)
        layers = []
        for stride in strides:
            layers.append(ResidualBlockNoRes(self.in_channels, out_channels, stride))
            self.in_channels = out_channels
        return nn.Sequential(*layers)



# 训练模型
def train(model, optimizer):
    model.train()
    running_loss = 0.0
    for i, data in enumerate(train_loader, 0):
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    meal_loss=running_loss/len(train_loader)
    return meal_loss
        #if i % 100 == 99:
            #print('[Epoch %d, Batch %5d] Loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))
            #running_loss = 0.0

def test(model):
    model.eval()
    correct = 0
    total = 0
    test_loss=0.0
    with torch.no_grad():
        for data in test_loader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            test_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    return 100*correct/total,test_loss/len(test_loader)


def train_model():
    logging.basicConfig(filename='train_without_residual.log', level=logging.INFO)

    for epoch in range(num_epochs):
        # train model with residual
        # meal_loss1 = train(model_with_residual, optimizer_with_residual)
        # accu1, test_loss1 = test(model_with_residual)
        # print("with residual epoch:%d  Train_Loss:%.3f Test_loss:%.3f Accuracy:%.3f" % (
        # epoch + 1, meal_loss1, test_loss1, accu1))
        # logging.info('Epoch %d: Train_Loss:%.3f Test_loss:%.3f Accuracy:%.3f', epoch + 1, meal_loss1,test_loss1, accu1)

        # train model without residual
        meal_loss2 = train(model_without_residual, optimizer_without_residual)
        accu2, test_loss2 = test(model_without_residual)
        # print("without residual epoch:%d  Loss:%.3f  Accuracy:%.2f" %(epoch+1,meal_loss2,accu2))
        logging.info('Epoch %d: Train_Loss:%.3f Test_loss:%.3f Accuracy:%.3f', epoch + 1, meal_loss2, test_loss2, accu2)
    # 保存模型
    # torch.save(model_with_residual.state_dict(), 'resnet18_with_residual.pth')
    torch.save(model_without_residual.state_dict(), 'resnet18_without_residual.pth')

def draw_image():
    with open('train_with_residual.log', 'r') as f:
        lines = f.readlines()
        loss_train_with = [float(line.split(' ')[-3].split(":")[-1]) for line in lines]
        loss_test_with = [float(line.split(' ')[-2].split(":")[-1]) for line in lines]
        accu_with = [float(line.split(' ')[-1].split(":")[-1]) for line in lines]

    with open('train_without_residual.log', 'r') as f:
        lines = f.readlines()
        loss_train_without = [float(line.split(' ')[-3].split(":")[-1]) for line in lines]
        loss_test_without = [float(line.split(' ')[-2].split(":")[-1]) for line in lines]
        accu_without = [float(line.split(' ')[-1].split(":")[-1]) for line in lines]

    x = [int(i) for i in range(0, 200)]

    plt.subplot(5, 1, 1)
    plt.plot(x, loss_train_with, color="r", label="model_with_residual")
    plt.plot(x, loss_train_without, color="b", label="model_without_residual")
    plt.legend()
    plt.title("Train_loss")

    plt.subplot(5, 1, 3)
    plt.plot(x, loss_test_with, color="r", label="model_with_residual")
    plt.plot(x, loss_test_without, color="b", label="model_without_residual")
    plt.legend()
    plt.title("Test_loss")

    plt.subplot(5, 1, 5)
    plt.plot(x, accu_with, color="r", label="model_with_residual")
    plt.plot(x, accu_without, color="b", label="model_without_residual")
    plt.legend()
    plt.title("Accuracy")

    plt.show()

if __name__ == '__main__':
    # 定义超参数
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    num_epochs = 200
    batch_size = 128
    learning_rate = 0.1
    momentum = 0.9
    weight_decay = 1e-4

    # 加载 CIFAR-10 数据集
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])
    ])


    train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)
    test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)

    # 定义模型、损失函数和优化器
    model_with_residual = ResNet18().to(device)
    model_without_residual = ResNet18NoRes().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer_with_residual = optim.SGD(model_with_residual.parameters(), lr=learning_rate, momentum=momentum,weight_decay=weight_decay)
    optimizer_without_residual = optim.SGD(model_without_residual.parameters(), lr=learning_rate, momentum=momentum,weight_decay=weight_decay)


    # model_with_residual.load_state_dict(torch.load("resnet18_with_residual.pth"))
    # model_without_residual.load_state_dict(torch.load("resnet18_without_residual.pth"))

    #训练模型
    #train_model()
    #绘图
    draw_image()

